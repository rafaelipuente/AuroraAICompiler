# AuroraAICompiler

![License](https://img.shields.io/badge/license-Apache%202.0-blue)
![Build](https://img.shields.io/badge/build-passing-brightgreen)
![Platform](https://img.shields.io/badge/platform-Linux%20%7C%20macOS-lightgrey)

AuroraAICompiler is a modular AI compiler stack designed for high-performance execution of neural network models. Built with MLIR, LLVM, and native runtime components, it transforms ONNX and PyTorch models into optimized machine code for efficient inference.

---

## Why this project?

This compiler was inspired by Ampere Computingâ€™s vision for AI-native silicon and their AmpereOne Aurora platform. The goal is to replicate the work of an AI Compiler Engineer at Ampereâ€”designing and delivering high-performance compilers for machine learning workloads using MLIR, LLVM IR, and real runtime integration.

---

## Overview

AuroraAICompiler converts deep learning models from frameworks like PyTorch and ONNX into optimized executables:

1. **Model Import** â€“ Parses ONNX or PyTorch model definitions
2. **Representation** â€“ Converts models to a custom [MLIR](https://mlir.llvm.org/) dialect for AI operations
3. **Optimization** â€“ Applies control/data flow compiler techniques (e.g., DCE, fusion, unrolling)
4. **Code Generation** â€“ Lowers to [LLVM IR](https://llvm.org/docs/LangRef.html) and generates optimized native code
5. **Execution** â€“ Supports JIT and AOT execution with a lightweight runtime
6. **Benchmarking** â€“ Measures performance vs reference implementations

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ONNX/PyTorch  â”‚â”€â”€â”€â”€â–¶â”‚ Aurora IR    â”‚â”€â”€â”€â”€â–¶â”‚ LLVM IR        â”‚â”€â”€â”€â”€â–¶â”‚ Optimized  â”‚
â”‚ Model Import  â”‚     â”‚ + Optimizers â”‚     â”‚ Generation     â”‚     â”‚ Executable â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Components

* **Model Importer** â€“ Python tools to ingest [ONNX](https://onnx.ai/) and [PyTorch](https://pytorch.org/) models
* **Aurora Dialect** â€“ Custom MLIR dialect for representing AI workloads (Conv2D, MatMul, etc.)
* **Optimization Pipeline** â€“ Applies domain-specific and general compiler passes
* **LLVM Code Generator** â€“ Lowers Aurora IR to LLVM IR and compiles to native code
* **Runtime Execution** â€“ Lightweight C++ engine for JIT/AOT compiled models
* **Benchmarking Tools** â€“ Scripts to measure performance vs baseline PyTorch or ONNXRuntime

---

## Prerequisites

* CMake 3.20+
* LLVM/MLIR 16.0+
* Python 3.8+
* C++17 compatible compiler
* ONNX 1.12+
* PyTorch 2.0+ (optional)

---

## Building the Compiler

```bash
# Clone the repository
git clone https://github.com/yourusername/AuroraAICompiler.git
cd AuroraAICompiler

# Configure and build
mkdir build && cd build
cmake ..
make -j$(nproc)

# Install (optional)
make install
```

---

## Running Examples

```bash
# Compile an ONNX model to Aurora IR
./tools/aurora-compile --input=models/resnet50.onnx --output=resnet50.aurora

# Run inference using the Aurora runtime
./tools/aurora-run --model=resnet50.aurora --input=sample_image.jpg
```

---

## ğŸ“Š Visual Debugging with Mermaid

AuroraAICompiler can generate operation graphs using Mermaid and render them as PNG diagrams for inspection.

Here's an example output from the compiler when run with `--dump-mermaid` and `--mermaid-png`:

![Aurora Compiler Mermaid Diagram](docs/test_model_diagram.png)

You can use:
```bash
./bin/aurora-compile test_model.mlir -o output.mlir --dump-mermaid --mermaid-png
```
to automatically generate this kind of visual output for any MLIR model.

### Mermaid Source

The raw Mermaid diagram source looks like this:

```mermaid
graph TD
  aurora.matmul["aurora.matmul\ntensor<8x8xf32>"]
  aurora.relu["aurora.relu\ntensor<8x8xf32>"]
  main_func["main\nfunc.func"]

  aurora.matmul --> aurora.relu
```

---

## Project Structure

* `include/` â€“ C++ header files (Dialects, Passes, Runtime)
* `src/` â€“ C++ implementations
* `python/` â€“ Python bindings + ONNX/PyTorch import logic
* `tools/` â€“ CLI tools: aurora-compile, aurora-run, aurora-benchmark
* `examples/` â€“ Usage examples
* `benchmarks/` â€“ Performance measurement
* `test/` â€“ Unit and integration tests
* `models/` â€“ Sample models (e.g. resnet50)
* `docs/` â€“ Developer documentation

---

## Benchmarks (In Progress)

| Model     | Baseline (ms) | AuroraCompiler (ms) | Speedup |
| --------- | ------------- | ------------------- | ------- |
| ResNet18  | 122.4         | 88.1                | 1.39Ã—   |
| Llama2-7B | TBD           | TBD                 | TBD     |

---

## Contributing

Contributions are welcome! Please check out [CONTRIBUTING.md](CONTRIBUTING.md) to get started.

---

## License

Licensed under the Apache License 2.0. See the [LICENSE](LICENSE) file for details.

---

## Related Tech

* [MLIR](https://mlir.llvm.org/)
* [LLVM](https://llvm.org/)
* [ONNX](https://onnx.ai/)
* [PyTorch](https://pytorch.org/)
* [vLLM](https://github.com/vllm-project/vllm)
